{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-26T14:21:48.974248Z",
     "start_time": "2018-02-26T14:21:48.711804Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sparsify' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-efb98251f997>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparsify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLeNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mnew_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sparsify' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch  \n",
    "import torch.nn as nn\n",
    "\n",
    "def compress_matrix(x):\n",
    "\n",
    "    if len(x.shape) != 2:\n",
    "        A, B, C, D = x.shape\n",
    "        x = x.reshape(A * B,  C * D)\n",
    "        ind = np.argwhere((np.sum(np.abs(x), axis=1)) == 0.)\n",
    "        # remove non-necessary filters and rows\n",
    "        x = x[:, (x != 0).any(axis=0)]\n",
    "        x = x[(x != 0).any(axis=1), :]\n",
    "        x = x.reshape(-1,B,C,D)\n",
    "    else:\n",
    "        # remove unnecessary rows, columns\n",
    "        x = x[(x != 0).any(axis=1), :]\n",
    "        x = x[:, (x != 0).any(axis=0)]\n",
    "        ind = numpy.array([0.]) # for now\n",
    "                          \n",
    "    return x, ind\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = sparsify(LeNet())\n",
    "\n",
    "new_channels = []\n",
    "\n",
    "for m in model.children():\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        num_channels = compress_layer(m)\n",
    "        new_channels.append(num_channels)\n",
    "\n",
    "compressed = TinyNet(channels=new_channels)\n",
    "\n",
    "for layer, compressed_layer in zip(model.children(), compressed.children()):\n",
    "    compressed_layer.weight = layer.weight\n",
    "    if(layer.bias):\n",
    "        compressed_layer.bias   = layer.bias\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "example = nn.Conv2d(3,6,5)\n",
    "\n",
    "print(example)\n",
    "\n",
    "x = example.weight.data.numpy()\n",
    "x[1,:] = 0.\n",
    "\n",
    "x, ind = compress_matrix(x)\n",
    "out_, in_, height_, width_ = x.shape\n",
    "\n",
    "new_layer = nn.Conv2d(in_, out_, height_)\n",
    "new_layer.weight.data = torch.Tensor(x)\n",
    "\n",
    "print(new_layer)\n",
    "\n",
    "## remove the bias\n",
    "\n",
    "\n",
    "# now need to remove the inputs corresponding to the channel we just pruned in the following layer\n",
    "# could be conv, bn, pool or fc\n",
    "# layer 2 of LeNet\n",
    "next_layer = nn.Conv2d(6,16,5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T09:36:10.885956Z",
     "start_time": "2018-02-21T09:36:10.856015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(2, 3, kernel_size=(2, 2), stride=(1, 1))\n",
      "Conv2d(2, 2, kernel_size=(2, 2), stride=(1, 1))\n",
      "[[[[-0.07139529  0.04683082]\n",
      "   [ 0.14715445  0.10146303]]\n",
      "\n",
      "  [[ 0.20623404 -0.20414823]\n",
      "   [ 0.01558907  0.11479891]]\n",
      "\n",
      "  [[ 0.26737356  0.16508116]\n",
      "   [ 0.01479433  0.11537937]]]\n",
      "\n",
      "\n",
      " [[[-0.03518109  0.1855519 ]\n",
      "   [ 0.25363138 -0.07190578]]\n",
      "\n",
      "  [[ 0.0926738   0.14208406]\n",
      "   [-0.14850877  0.00415868]]\n",
      "\n",
      "  [[ 0.05830792 -0.13324894]\n",
      "   [ 0.10483494  0.07671355]]]\n",
      "\n",
      "\n",
      " [[[-0.23851964  0.1788559 ]\n",
      "   [ 0.11026009 -0.01322468]]\n",
      "\n",
      "  [[ 0.15628926 -0.22298165]\n",
      "   [ 0.02409538 -0.24568257]]\n",
      "\n",
      "  [[ 0.07694096  0.00656321]\n",
      "   [-0.2602202  -0.14715835]]]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[2]\n",
      " [3]]\n",
      "[[[[-0.07139529  0.04683082]\n",
      "   [ 0.14715445  0.10146303]]\n",
      "\n",
      "  [[ 0.20623404 -0.20414823]\n",
      "   [ 0.01558907  0.11479891]]\n",
      "\n",
      "  [[ 0.26737356  0.16508116]\n",
      "   [ 0.01479433  0.11537937]]]\n",
      "\n",
      "\n",
      " [[[-0.03518109  0.1855519 ]\n",
      "   [ 0.25363138 -0.07190578]]\n",
      "\n",
      "  [[ 0.0926738   0.14208406]\n",
      "   [-0.14850877  0.00415868]]\n",
      "\n",
      "  [[ 0.05830792 -0.13324894]\n",
      "   [ 0.10483494  0.07671355]]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jackturner/miniconda3/envs/mlp/lib/python3.6/site-packages/ipykernel_launcher.py:24: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n"
     ]
    }
   ],
   "source": [
    "l1 = nn.Conv2d(2,3,2)\n",
    "print(l1)\n",
    "l1 = l1.weight.data.numpy()\n",
    "l1[1,:] = 0. \n",
    "l1, ind = compress_matrix(l1)\n",
    "out_, in_, height_, width_ = l1.shape\n",
    "\n",
    "new_layer = nn.Conv2d(in_, out_, height_)\n",
    "new_layer.weight.data = torch.Tensor(l1)\n",
    "\n",
    "print(new_layer)\n",
    "\n",
    "\n",
    "\n",
    "# follow up \n",
    "x = nn.Conv2d(3,3,2)\n",
    "x = x.weight.data.numpy()\n",
    "\n",
    "print(x)\n",
    "\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "print(ind)\n",
    "x = np.delete(x, ind, 0)\n",
    "\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T12:11:09.019230Z",
     "start_time": "2018-02-22T12:11:09.001117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-0.11309339 -0.13252267]\n",
      "   [ 0.10189015  0.23333943]]\n",
      "\n",
      "  [[ 0.18838969 -0.20607772]\n",
      "   [-0.04675874 -0.2579743 ]]\n",
      "\n",
      "  [[-0.22457686 -0.12578286]\n",
      "   [ 0.10464424 -0.02261609]]]\n",
      "\n",
      "\n",
      " [[[ 0.21070072  0.12532869]\n",
      "   [-0.2725599  -0.10313283]]\n",
      "\n",
      "  [[-0.13886876 -0.2513148 ]\n",
      "   [-0.25815955  0.2037502 ]]\n",
      "\n",
      "  [[ 0.18243036  0.06070128]\n",
      "   [ 0.0784964  -0.14904013]]]] \n",
      "\n",
      "--------\n",
      "[[[ 0.21070072  0.12532869]\n",
      "  [-0.2725599  -0.10313283]]\n",
      "\n",
      " [[-0.13886876 -0.2513148 ]\n",
      "  [-0.25815955  0.2037502 ]]\n",
      "\n",
      " [[ 0.18243036  0.06070128]\n",
      "  [ 0.0784964  -0.14904013]]] \n",
      "\n",
      "---------\n",
      "[[[ 0.18838969 -0.20607772]\n",
      "  [-0.04675874 -0.2579743 ]]\n",
      "\n",
      " [[-0.13886876 -0.2513148 ]\n",
      "  [-0.25815955  0.2037502 ]]] \n",
      "\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "x = nn.Conv2d(3,2,2).weight.data.numpy()\n",
    "\n",
    "\n",
    "print(x, \"\\n\\n--------\")\n",
    "\n",
    "\n",
    "print(x[1,:,:,:], \"\\n\\n---------\")\n",
    "print(x[:,1,:,:], \"\\n\\n---------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Channel Pruning in PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-26T14:22:12.562160Z",
     "start_time": "2018-02-26T14:22:11.926015Z"
    },
    "code_folding": [
     48,
     85
    ]
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Conv2d' object has no attribute 'kernel_width'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2f777871ea4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLeNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;31m#sparsify(model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m \u001b[0mchannels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompress_convs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-2f777871ea4c>\u001b[0m in \u001b[0;36mcompress_convs\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0mprune_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnonzeros\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                 \u001b[0mchannel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_width\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_height\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m                 \u001b[0mprune_fc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_conv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm2d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlp/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    364\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 366\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Conv2d' object has no attribute 'kernel_width'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch  \n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.conv3 = nn.Conv2d(16, 16, 5)\n",
    "        self.conv4 = nn.Conv2d(16, 10, 5)\n",
    "        self.fc1   = nn.Linear(10*5*5, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv3(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv4(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        \n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class TinyNet(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(TinyNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, channels[0], 5)\n",
    "        self.conv2 = nn.Conv2d(channels[0], channels[1], 5)\n",
    "        self.conv3 = nn.Conv2d(channels[1], channels[2], 5)\n",
    "        self.conv4 = nn.Conv2d(channels[2], channels[3], 5)\n",
    "        self.fc1   = nn.Linear(channels[3] * 5 * 5, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv3(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv4(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        \n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "def calculate_threshold(weights, ratio):\n",
    "    return np.percentile(np.array(torch.abs(weights)), ratio)\n",
    "\n",
    "def sparsify(model, threshold=50.):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            threshold  = calculate_threshold(param.data, threshold)\n",
    "            mask       = torch.gt(torch.abs(param), threshold).float()\n",
    "            param.data = (param * mask).data\n",
    "    return model\n",
    "\n",
    "def sparsify_on_bn(model):\n",
    "    '''\n",
    "    Here we zero out whole planes where their batchnorm weight is 0\n",
    "    1. Consider lists in pairs\n",
    "    2. If conv followed by batchnorm - get nonzeros from batchnorm \n",
    "    3. Zero out whole conv filters\n",
    "    '''\n",
    "    \n",
    "    for l1, l2 in zip(list(model.children()), list(model.children())[1:]):\n",
    "        if isinstance(l1, nn.Conv2d) and isinstance(l2, bn.BatchNorm2dEx):\n",
    "            zeros = argwhere_nonzero(l2.weight, batchnorm=True)\n",
    "            l1[zeros] = 0.\n",
    "            \n",
    "\n",
    "def argwhere_nonzero(layer, batchnorm=False):\n",
    "    indices=[]\n",
    "    \n",
    "              \n",
    "    # for batchnorms we want to do the opposite\n",
    "    if batchnorm:\n",
    "        x = layer.data.cpu().numpy()\n",
    "        indices = np.argwhere(x, x==0.) # <<- not sure about syntax\n",
    "    else:\n",
    "        for idx,w in enumerate(layer):\n",
    "            if torch.sum(torch.abs(w)) != 0.:\n",
    "                indices.append(idx)\n",
    "        \n",
    "    return indices\n",
    "\n",
    "\n",
    "def prune_conv(indices, layer, follow=False):\n",
    "    # follow tells us whether we need to prune input channels or output channels\n",
    "    if not follow:\n",
    "        # prune output channels\n",
    "        layer.weight.data = layer.weight[indices].data\n",
    "        layer.bias.data   = layer.bias[indices].data\n",
    "    else: \n",
    "        # prune input channels\n",
    "        layer.weight.data = layer.weight[:,indices].data\n",
    "        \n",
    "def prune_fc(indices, channel_size, layer, follow_conv=True):\n",
    "    if follow_conv:\n",
    "        # if we are following a conv layer we need to expand each index by the size of the plane\n",
    "        indices = [item for sublist in list((map(lambda i : np.arange(i, (i+channel_size)), indices))) for item in sublist]\n",
    "    \n",
    "    fc_layer[indices]\n",
    "        \n",
    "def compress_convs(model):\n",
    "    \n",
    "    ls = list(model.children())\n",
    "\n",
    "    channels = []\n",
    "    nonzeros = []\n",
    "    for l1, l2 in zip(ls, ls[1:]):\n",
    "        # so now we have pairs of layers\n",
    "        \n",
    "        if isinstance(l1, nn.Conv2d):\n",
    "            nonzeros = argwhere_nonzero(l1.weight)\n",
    "            channels.append(len(nonzeros))\n",
    "            \n",
    "            prune_conv(nonzeros, l1)\n",
    "            \n",
    "            if isinstance(l2, nn.Conv2d):\n",
    "                prune_conv(nonzeros, l2, follow=True)\n",
    "            elif isinstance(l2, nn.Linear):\n",
    "                channel_size = l1.kernel_size[0] * l1.kernel_size[1]\n",
    "                prune_fc(indices, channel_size, l2, follow_conv=True)\n",
    "            elif isinstance(l2, nn.BatchNorm2d):\n",
    "                prune_fc(indices, 0, l2, follow_conv=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    new_model = TinyNet(channels)\n",
    "    \n",
    "    for original, compressed in zip(model.children(), new_model.children()):\n",
    "        compressed.weight = original.weight\n",
    "        compressed.bias   = original.bias\n",
    "    \n",
    "    return new_model\n",
    "            \n",
    "\n",
    "model = LeNet()    \n",
    "#sparsify(model)\n",
    "channels = compress_convs(model)\n",
    "\n",
    "\n",
    "new_model = TinyNet(channels)\n",
    "\n",
    "for original, compressed in zip(model.children(), new_model.children()):\n",
    "    compressed.weight = original.weight\n",
    "    compressed.bias   = original.bias\n",
    "    \n",
    "new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fclayer = [0,1,2,3,4,5]\n",
    "\n",
    "indices = [0]\n",
    "channel_size=3\n",
    "\n",
    "indices = [item for sublist in list((map(lambda i : np.arange(i, (i+channel_size)), indices))) for item in sublist]\n",
    "\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
